#!/bin/bash
#Script to submit to a GCloud dataproc cluster. Copy this file to submit-gcloud.sh and 
#edit variables accordingly.

#Change the region if your cluster is not in asia-northeast1
REGION=asia-northeast1

#The first argument is the cluster ID. The remaining arguments will be passed to the Discount driver process.
CLUSTER=$1
shift

MAXRES=##spark.driver.maxResultSize=2g

#High memory
PARTITIONS=##spark.sql.shuffle.partitions=4000
#Low memory
#PARTITIONS=##spark.sql.shuffle.partitions=12000

#YARN memory is allocated using on GCP using executor memory and memoryOverhead.
#The number of executors that will be spawned by YARN is (total memory)/(executor memory + memoryOverhead).
#Furthermore, on GCP every executor runs two task threads by default.
#Below are some suggested settings for EXECMEM and OVERHEAD. It is also safe to leave them blank,
#in which case the GCP environment will supply some default settings.

#The two settings below suitable for k-mer counting on highcpu 16-core nodes
#They also for standard 4-core nodes.
#OVERHEAD=##spark.executor.memoryOverhead=768
#EXECMEM=##spark.executor.memory=4352m

#Half memory setting for std-16 nodes. Artificially inflated overhead to limit #executors
#OVERHEAD=##spark.executor.memoryOverhead=$((11171 + 1117))
#EXECMEM=##spark.executor.memory=11171m

#The special characters at the start make '##' the separator of properties in the list, since the comma sign
#is needed for spark.jars.packages.
#spark.executor.memory=8g is suitable for highmem nodes on GCloud.
PROPERTIES="^##^spark.jars.packages=org.rogach:scallop_2.11:latest.integration$PARTITIONS$MAXRES$OVERHEAD$EXECMEM"

exec gcloud --verbosity=info  dataproc jobs submit spark --region $REGION --cluster $CLUSTER \
  --class discount.spark.Discount --jars target/scala-2.11/discount_2.11-1.0.0-SNAPSHOT.jar,lib/fastdoop-1.0.0.jar \
  --properties $PROPERTIES -- "$@"


