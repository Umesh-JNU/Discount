#!/bin/bash
#Copy this file to spark-submit.sh and edit the config variables.

MASTER=local[*]
#If you are running a standalone cluster, use the following instead
#MASTER=spark://localhost:7077

SPARK=/path/to/spark-2.4.X-bin-hadoopX.X

#To reduce memory usage for big inputs, increase PARTITIONS
#PARTITIONS="spark.sql.shuffle.partitions=4000"
PARTITIONS="spark.sql.shuffle.partitions=400"

#Max size of input splits in bytes. A smaller number reduces memory usage but increases the number of 
#partitions for the first stage. If this variable is unset, Spark's default of 128 MB will be used.
SPLIT="spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=$((64 * 1024 * 1024))"

#Change 2.11 to 2.12 in all three places below if compiling for scala 2.12.
exec $SPARK/bin/spark-submit \
  --conf spark.driver.maxResultSize=2g \
  --conf $PARTITIONS \
  --conf $SPLIT \
  --packages org.rogach:scallop_2.11:latest.integration \
  --jars lib/fastdoop-1.0.0.jar \
  --master $MASTER \
  --class discount.spark.Discount target/scala-2.11/discount_2.11-1.2.0.jar $*
